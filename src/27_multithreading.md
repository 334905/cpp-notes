# Многопоточность

- [Запись лекции №1](https://www.youtube.com/watch?v=q9A2nvqW0QQ)
- [Запись лекции №2](https://www.youtube.com/watch?v=ebb2IYxlSPU)
- [Запись лекции №3](https://www.youtube.com/watch?v=DvBpHBsdWew)
- [C++ and Beyond 2012: Herb Sutter - atomic Weapons 1 of 2](https://www.youtube.com/watch?v=A8eCGOqgvH4)
- [C++ and Beyond 2012: Herb Sutter - atomic Weapons 2 of 2](https://www.youtube.com/watch?v=KeLBd2EJLOU)
- [Запись лекции №4](https://www.youtube.com/watch?v=RtQqEre0Ag8)
- [Запись лекции №5](https://www.youtube.com/watch?v=HcIaqm9b5hU)
- [CppCon 2019: Olivier Giroux “The One-Decade Task: Putting std::atomic in CUDA.”](https://www.youtube.com/watch?v=VogqOscJYvk)
- [Запись лекции №6](https://www.youtube.com/watch?v=SiGqozCBXDE)

## Мотивация.

**Поток исполнения** *— независимая последовательность выполнения инструкций внутри одного процесса с общей памятью, но собственными регистрами и стеком.*

Банальное применение — чтобы использовать несколько имеющихся ядер.

Стоит отметить, что нагрузить ядра можно не только несколькими потоками, но и несколькими процессами (так компиляторы работают: потоков у них нет, а параллельность есть).\
Кроме того, API для работы с несколькими потоками появился ещё до того, как процессоры с несколькими ядрами стали популярными — получается, что потребность в потоках возникла не только для нагрузки ядер. Одна из потребностей заключается в использовании потоков для функций, которые должны работать параллельно основной программе (запускаем какую-нибудь функцию из библиотеки, но не хотим, чтобы она была блокирующей, например, если параллельно нужно читать ввод от пользователя).

Многопоточность в терминах C++ довольно важна не только для того, чтобы знать, как она есть, но и, например, чтобы понимать, почему аллокаторы или shared pointer'ы работают так, как работают, так как они сильно связаны с дизайном многопоточки.

API для работы с потоками в C++ появился в 11-м стандарте и был расширен в 20-м. До этого стандарт не предоставлял ничего для работы с многопоточными программами, поэтому люди делали всё средствами ОС.

## Простые примеры работы с потоками.

```c++
int main() {
    std::thread th([]{
        std::cout << "Hello, world!\n";
    });
    th.join();
}
```

В стандартной библиотеке есть класс `std::thread`. В конструкторе он стартует поток и потом можно дождаться, пока он закончит (при помощи `join`).\
Помимо этого есть операция `detach`: она отвязывает наш `std::thread` от потока ОС, но поток ОС никто не завершает. Такое бывает нужно очень редко. Например, вот такая программа просто некорректна:

```c++
int main() {
    std::thread th([]{
        std::cout << "Hello, world!\n";
    });
    th.detach();
}
```

После выхода из `main` всё равно все потоки будут убиты при завершении программы, но кроме того, при выходе из `main` вызываются деструкторы глобальных переменных (например, `std::cout`), к которым может обращаться поток, поэтому делать `detach` не очень хорошо. В принципе, `detach` нужен вообще достаточно редко, потому что в основном поток привязан к каким-то глобальным данным (как в примере выше, к глобальной переменной `std::cout`). 

А что же будет, если мы не напишем ни `join`, ни `detach`? Вызовется деструктор `std::thread`, который аварийно завершит программу вызовом `std::terminate`. Зачем? А потому что у нас бывают потоки, которые работают бесконечно долго. Поэтому по умолчанию сделать `join` — не очень затея. Но если вы всё же хотите `join` по-умолчанию, то ваш выбор: `std::jthread` — он в деструкторе делает `join`.\
У него ещё есть способ сказать потоку «так, ты это, выходи пожалуйста», и после этого сделать `join`, но это мы обсудим [позже](#cancellation).

Однако если поток не хочет с вами кооперироваться и выполнять вашу просьбу о завершении, то вы проиграли. Способа принудительно завершить поток в стандартной библиотеке нет. Если бы такой способ был, то он бы завершал поток в неопределённой точке, что приводило бы к неконсистентными данным: например, если поток делал какие-то операции со структурой данных, она в таком случае осталась бы в некорректном состоянии.

Однако механизмы завершения есть у ОС: например, в Windows есть `TerminateThread`, но в описании всех таких механизмов написано, что они «опасная функция, которую стоит использовать в совсем экстренных случаях». Например, в `TerminateThread` написано следующее:

> You should call `TerminateThread` only if you know exactly what the target thread is doing, and you control all of the code that the target thread could possibly be running at the time of the termination.

Если вы убьёте поток в критической секции, вы проиграли. Если вы убьёте поток в тот момент, когда он выделял память, то вы совсем больше не сможете выделять память в вашей программе.

## Проблемы с жизнью в многопоточной среде и попытки их решить.

### Гоночки.

Пусть мы делаем банковское приложение и у нас есть 10000 счетов, а умеем мы переводить деньги с одного на другой.

```c++
std::array<uint32_t, 10'000> accounts;

void transfer(size_t to, size_t from, uint32_t amount) {
    if (accounts[from] < amount) {
        throw std::runtime_error("insufficient funds");
    }
    accounts[from] -= amount;
    accounts[to] += amount;
}
```

Чудесная функция для однопоточного случая. И даже если не иметь проблем с `+=` и `-=`, то мы можем переводить деньги с одного счёта на два разных, и тогда мы можем два раза подряд проверить, потом два раза вычесть. А денег изначально только на один перевод хватало. Ой.\
Ещё мы не знаем, как работает `+=`. Это может быть единая неделимая операция, а может быть сначала чтение, потом запись. И тут у нас ещё одна гонка: когда мы сначала читаем два раза, потом оба вычитают и записывают. И эффект от одного `+=` не применяется.

Это чудо называется **состояние гонки**, **race condition** Если воспринимать это чудо как псевдокод, если же написать это честно на C++, то это жёсткое UB.

### `std::mutex` и его друзья.

Нам может помочь примитив синхронизации под названием **mutex** (от слов *mutual exclusion* — взаимное исключение). В стандартной библиотеке это класс `std::mutex` с двумя операциями: `lock` и `unlock`. У него может быть два состояния: разблокирован (по умолчанию при создании) и заблокирован. Вызов `lock` ждёт, пока `mutex` будет разблокирован, а потом блокирует его. Метод `unlock`, очевидно, разблокирует `mutex`. 

```c++
std::mutex m;
std::array<uint32_t, 10'000> accounts;

void transfer(size_t to, size_t from, uint32_t amount) {
    m.lock();
    if (accounts[from] < amount) {
        m.unlock();
        throw std::runtime_error("insufficient funds");
    }
    accounts[from] -= amount;
    accounts[to] += amount;
    m.unlock();
}
```

Что важно понимать: `mutex` идеологически ассоциирован с данными, а не функциями. Если бы мы написали ещё какую-то операцию с `accounts`, там надо было бы тоже работать с `mutex`'ом.

Ещё можно заметить, что мы тут делаем `unlock` при обоих выходах из функции, а это, как мы знаем, обычно делается при помощи [RAII](./08_exceptions.md#raii). И в стандартной библиотеке есть `std::lock_guard` — RAII-обёртка над `mutex`'ом, которая в конструкторе блокирует его, а в деструкторе разблокирует:

```c++
std::mutex m;
std::array<uint32_t, 10'000> accounts;

void transfer(size_t to, size_t from, uint32_t amount) {
    std::lock_guard<std::mutex> lock(m);

    if (accounts[from] < amount)
        throw std::runtime_error("insufficient funds");

    accounts[from] -= amount;
    accounts[to] += amount;
}
```

Кстати, `std::lock_guard<std::mutex>` в данном коде можно заменить на просто `std::lock_guard` (начиная с C++17). Это называется [CTAD (class template argument deduction)](https://en.cppreference.com/w/cpp/language/class_template_argument_deduction), но это сейчас не важно.

### Закон Амдала.

Проблема такого примера в том, что мы блокируем все аккаунты одновременно, лишаясь параллельности. С одной стороны, это плохо, с другой, функция `transfer` не обязательна должна уметь параллельно работать с несколькими аккаунтами — например, если программа в несколько потоков делает кучу другой работы, но иногда вызывает `transfer`.\
Здесь можно сослаться на известный [Закон Амдала](https://en.wikipedia.org/wiki/Amdahl%27s_law), который заключается в следующем: у нас есть части кода, которые параллелятся, и части, которые нет. И чем больше процент первого, тем больше ускорение программы мы можем получить. Если у нас 50% программы можно распараллелить, то как много у нас потоков не было бы, ускорение больше чем вдвое мы не получим никак.\
Хотя на самом деле стоит ещё учитывать то, что чем больше у нас потоков, тем больше накладных расходов у нас есть.

### Взаимоблокировки (a.k.a. Deadlock).

Допустим, что нам хочется, чтобы независимые по данным вызовы функции `transfer` могли работать параллельно. Можно создать больше мьютексов! Такой приём называется **мелкогранулярными блокировками**.

```c++
struct account {
    std::mutex m;
    uint32_t balance;
};

std::array<account, 10'000> accounts;

void transfer(size_t to, size_t from, uint32_t amount) {
    std::lock_guard lock_from(accounts[from].m);
    std::lock_guard lock_to(accounts[to].m);

    if (accounts[from].balance < amount)
        throw std::runtime_error("insufficient funds");

    accounts[from].balance -= amount;
    accounts[to].balance += amount;
}
```

И тут мы сталкиваемся с ситуацией, которая называется **deadlock**: при вызовах `transfer(a, b, 42)`, `transfer(b, a, 42)` может произойти так, что первый поток заблокирует мьютекс `a`, второй заблокирует `b`, и в итоге первый поток будет бесконечно ждать `b`, который заблокирован вторым потоком, который бесконечно ждёт `a`, в итоге программа зависнет.

Формально, **deadlock** *— это ситуация, при которой ни один поток находится в ожидании ресурсов, занятых друг другом, и не может продолжить своё исполнение*.

Конкретно тут возникает традиционная проблема, которая называется проблемой обедающих философов. Если у нас цикл из `transfer`'ов и каждый успеет взять первый нужный ему `mutex`, но не успеет второй, то будет deadlock.

Это решается введением порядка на `mutex`'ах — назначим им номера и введём на них отношения порядка, и всегда будем блокировать сначала более ранний, потом более поздний `mutex`. Например, в примере выше номерами могут быть номера аккаунтов, а блокировать можно всегда меньший.

```c++
void transfer(size_t from, size_t to, uint32_t amount) {
    std::lock_guard lock1(accounts[std::min(from, to)].m);
    std::lock_guard lock2(accounts[std::max(from, to)].m);

    if (accounts[from] < amount)
        throw std::runtime_error("insufficient funds");

    accounts[from] -= amount;
    accounts[to] += amount;
}
```

Почему блокировать в каком-то порядке всегда это верно? Можно представить ориентированный граф, в котором вершины это `mutex`'ы, а ребро `uv` означает, что в программе есть место, в котором, удерживая мьютекс `u`, поток попытается заблокировать мьютекс `v`. Утверждается, что если в этом графе нет циклов, то взаимоблокировка возникнуть не может. Если же на `mutex`'ах введён порядок, то циклы возникнуть не могут, так как все рёбра будут проведены от меньшего к большему.

Ещё одна проблема кода выше — мьютекс на каждый аккаунт заводить дорого, так как `sizeof` `std::mutex`'а — это порядка 40 байтов. Можно, к примеру, завести по `mutex`'у на какой-то отрезок аккаунтов.

### `std::recursive_mutex`.

Осталась одна проблема с нашим кодом: когда поток уже владеет `std::mutex`'ом и пытается получить его ещё раз, это UB.

```c++
void transfer(size_t from, size_t to, uint32_t amount) {
    if (from == to)
        return;

    std::lock_guard lock1(accounts[std::min(from, to)].m);
    std::lock_guard lock2(accounts[std::max(from, to)].m);

    if (accounts[from] < amount)
        throw std::runtime_error("insufficient funds");

    accounts[from] -= amount;
    accounts[to] += amount;
}
```

В данном случае всё, конечно, легко правится, но вообще, иногда нам очень хочется уметь брать `mutex` несколько раз, и в стандартной библиотеке есть такой вид `mutex`'ов: `std::recursive_mutex`.

А вообще в стандартной библиотеке есть [много интересных `mutex`'ов](https://en.cppreference.com/w/cpp/thread#Mutual_exclusion), и ещё больше есть в сторонних библиотеках.

### Другие методы решения проблемы взаимоблокировок.

Но проблему с взаимоблокировками можно решить и иначе. Проблема возникала из-за того, что мы блокировали один `mutex` и ждали разблокировки следующего, не отпуская заблокированный. Если бы мы, например, блокировали один, затем пробовали заблокировать второй, если же не получилось, отпускали первый и повторяли всё заново, то это позволило бы решить проблему (на самом деле не совсем, можете послушать про livelock на лекциях по операционным системам light, но не суть).\
У `std::mutex` есть метод `try_lock`, который возвращает `false` для заблокированного `mutex`'а, иначе блокирует его и возвращает `true`. 

Кроме того, в стандартной библиотеке есть функция `std::lock`, которая блокирует несколько объектов, имея при этом механизм обхода взаимоблокировок. Как она это делает — зависит от реализации, но вообще там некоторая неспецифицированная последовательность вызовов функций `lock`, `unlock`, `try_lock`.\
В таком случае нельзя использовать `std::lock_guard`, потому что он в конструкторе пытается заблокировать `mutex`, но можно `std::scoped_lock`.

А ещё можно воспользоваться `std::unique_lock`, который имеет более тонкую настройку (в нашем случае хочется не блокировать `mutex` в конструкторе, а блокировать потом через `std::lock`, а в деструкторе всё равно освобождать):

```c++
void transfer(size_t to, size_t from, uint32_t amount) {
    if (from == to)
        return;

    std::unique_lock<std::mutex> lock_from(accounts[from].m, std::defer_lock);
    std::unique_lock<std::mutex> lock_to(accounts[to].m, std::defer_lock);
    std::lock(lock_from, lock_to);
    // ...
}
```

## Другие примитивы синхронизации на примере написания многопоточной очереди.

Представьте, что у нас есть какие-то многопоточные запросы, и мы хотим поддерживать очередь из них.

```c++
template <class T>
struct concurrent_queue {
private:
    std::mutex m;
    std::deque<T> queue;

public:
    void push(T val) {
        std::lock_guard lock(m);
        queue.push_back(std::move(value));
    }

    bool empty() const {
        std::lock_guard lock(m);
        return queue.empty();
    }

    T pop() {
        std::lock_guard lock(m);
        T result = queue.back();
        queue.pop();
        return result;
    }
};
```

Нормально ли такое? Да нифига. Потому что мы не можем вызывать `empty` и сразу после этого `pop`.\
Потому что между этими вызовами у нас нет замка. Поэтому правильнее будет изменить интерфейс.

```c++
template <class T>
struct concurrent_queue {
private:
    std::mutex m;
    std::deque<T> queue;

public:
    void push(T val) {
        std::lock_guard lock(m);
        queue.push_back(std::move(value));
    }

    std::optional<T> try_pop() {
        std::lock_guard lock(m);
        if (queue.empty())
            return std::nullopt;
        std::optional<T> result = queue.back();
        queue.pop();
        return result;
    }
};
```

В чём тут ещё проблема? В том, что у нас есть чуваки, которые сидят и ждут, пока будут данные.\
Они не хотят писать `while`, это активное ожидание. Поэтому можно сделать `pop`, который ждёт.

### `std::condition_variable`

Появляется желание уметь "усыпить поток" и потом "пробудить его" из другого потока: для этого существует специальный класс `std::condition_variable`.  У него есть 3 операции: `wait`, `notify_one` и `notify_all`. Первая ждёт, вторая пробуждает одного ждущего, третья — пробуждает всех.\
Пока что наш код не будет компилироваться, но давайте всё равно его напишем:

```c++
template <class T>
struct concurrent_queue {
private:
    std::mutex m;
    std::deque<T> queue;
    std::condition_variable cv;

public:
    void push(T val) {
        std::lock_guard lock(m);
        queue.push_back(std::move(value));
        cv.notify_one();
    }
    T pop() {
        std::lock_guard lock(m);
        if (queue.empty())
            cv.wait(); // compilation error.
        T result = queue.back();
        queue.pop();
        return result;
    }
};
```

Но тут есть проблема: мы спим с `mutex`'ом. Пытаемся исправить:

```c++
T pop() {
    std::unique_lock lock(m);
    if (queue.empty()) {
        lock.unlock()
        cv.wait();
        lock.lock();
    }
    T result = queue.back();
    queue.pop();
    return result;
}
```

И тут уже сильно лучше, но если один поток пробудился и ещё не взял блокировку, а другой тем временем зашёл, увидел непустую очередь и опустошил её. И всё, мы проиграли. Поэтому вместо `if`'а надо написать `while`. Кстати, есть ещё одна причина, почему `while`: [спонтанные пробуждения](https://en.wikipedia.org/wiki/Spurious_wakeup). `condition_variable` может сам внезапно пробудиться ни с чего.

```c++
T pop() {
    std::unique_lock lock(m);
    while (queue.empty()) {
        lock.unlock()
        cv.wait();
        lock.lock();
    }
    T result = queue.back();
    queue.pop();
    return result;
}
```

И эту проблемы мы починили, но ещё есть другая: если мы уже сделали `unlock`, нам в это время сделали `push`, но    `notify_one` произошёл до того, как мы стали ждать. И всё, мы ждём при непустой очереди. Поэтому `wait` на самом деле принимает один аргумент: `unique_lock`, чтобы сделать `unlock`
и `wait` неделимо:

```c++
T pop() {
    std::unique_lock lock(m);
    while (queue.empty())
        cv.wait(lock);
    T result = queue.back();
    queue.pop();
    return result;
}
```

На самом деле такой паттерн с `while`'ом настолько частый, что под это специальная перегрузка есть:

```c++
T pop() {
    std::unique_lock lock(m);
    cv.wait(lock, [this]{ return !queue.empty(); });
    T result = queue.back();
    queue.pop();
    return result;
}
```

### Thundering herd problem.

Общего развития ради: не надо использовать `notify_all` там, где можно `notify_one`. Такое даже название имеет: [Thundering herd problem](https://en.wikipedia.org/wiki/Thundering_herd_problem).

### Worst practices.

Может показаться, что `notify` — штука долгая, и её можно пытаться оптимизировать. Давайте не будем никого уведомлять, если очередь не была пуста.

```c++
void push(T value) {
    std::lock_guard<std::mutex> lock(m);
    bool was_empty = queue.empty();
    queue.push_back(std::move(value));
    if (was_empty) {
        cv.notify_one();
    }
}
```

Проблема в том, что если ждут несколько `pop`, то может произойти следующее: в очередь произойдёт несколько вставок подряд, только первая сделает `notify`, а значит пробудится только один `pop`, притом что очередь будет не пуста.

### Best practices.

Если производителей данных больше, чем потребителей, очередь может бесконечно расти. Давайте ненадо. Ну, тривиально правится: заводим ещё `std::conditional_variable`, который будем ждать в `push` и сообщать в `pop`. Это, кстати, объясняет, зачем нам отдельно `mutex`'ы и отдельно `condition_variable`.

## Реализация.

### Worst practices.

Во-первых, не надо делать `std::this_thread::yield`. Никогда. Любой хоть сколько-нибудь продвинутый планировщик старается минимально мигрировать задачи между ядрами (кэши охлаждаются, например). Поэтому планировщики могут игнорировать `yield`. Зачем эта функция существует — ничуть не для примитивов синхронизации, а только если вы знаете всё про
планировщик и про приоритеты.

А что использовать для реализации? То, что ожидает ОС, объективно. И то что разработчики ОС рекомендуют. В POSIX есть такая штука как [futex](https://en.wikipedia.org/wiki/Futex) (который говорит, что поток будет ждать "по какой-то переменной/адресу в памяти" и пробуждает другой поток), и всё реализуется через него.

### Busy wait/spinlock.

Бывают случаи, когда засыпать потоку может быть невыгодно и можно просто немного подождать, пока `mutex` не разблокируется. Для этого применяется примитив синхронизации под названием **spinlock**, который просто крутится в цикле, проверяя, не заблокирован ли. Большинство реализацией `mutex`'ов совмещают эти подходы: крутятся в цикле какое-то время, а потом засыпают, если мьютекс не разблокировался.

Кстати, в многих процессорах есть специальные инструкции-подсказки для ОС. Например, на x86 есть [специальная инструкция `PAUSE` (*Spin Loop Hint*)](https://www.felixcloutier.com/x86/pause), которую можно вставить внутрь цикла spinlock'а, чтобы оно как-нибудь оптимизировалось.

### Ещё пример про взаимоблокировку.

```c++
struct integer_generator {
    integer_generator(std::function<void (uint64_t)> on_next_number, uint64_t val)
        : on_next_number(std::move(on_next_number))
        , current_val(val)
        , quit(false)
        , inner_thread([this] { thread_proc(); }) {}

    integer_generator(integer_generator const&) = delete;
    integer_generator& operator=(integer_generator const&) = delete;

    ~integer_generator(){
        {
            std::lock_guard<std::mutex> lock(m1);
            quit = true;
        }
        inner_thread.join();
    }

    void reset(uint64_t val) {
        std::lock_guard<std::mutex> lock(m1);
        current_val = val;
    }
private:
    void thread_proc() {
        for (;;) {
            std::lock_guard<std::mutex> lock(m1);
            if (quit)
                break;
            current_val++;
            on_next_number(current_val);
        }
    }
    
    std::mutex m1;
    std::function<void (uint64_t)> const on_next_number;
    uint64_t current_val;
    bool quit;
    std::thread inner_thread;
};

struct integer_accumulator
{
    integer_accumulator(uint64_t sum, uint64_t val)
        : current_sum(sum)
        , gen([this] (uint64_t next) {
            std::lock_guard<std::mutex> lock(m2);
            current_sum += next;
            std::cout << "sum = " << current_sum << ", " << next << '\n';
        }, val) {}

    void reset(uint64_t sum, uint64_t val) {
        std::lock_guard<std::mutex> lock(m2);
        current_sum = sum;
        gen.reset(val);
    }

private:
    std::mutex m2;
    uint64_t current_sum;
    integer_generator gen;
};

int main() {
    integer_accumulator acc(0, 0);
    for (;;) {
        std::this_thread::sleep_for(std::chrono::milliseconds(1));
        acc.reset(0, 0);
    }
}
```

Класс `integer_generator` хранит внутри себя поток `inner_thread`, который создаётся в конструкторе и крутится, пока не `quit`, вызывая функцию `on_next_number` с увеличивающимся каждый раз значением. Функция `reset` сбрасывает текущее значение. В деструкторе `quit` присваивается `true` и ждёт завершения потока.

Класс `integer_accumulator` создаёт генератор, передавая функцию для суммирования чисел.

В примере взаимоблокировка происходит в следующем случае: `acc.reset` берёт `mutex` `m2` и, удерживая его, заходит в `gen.reset` и пытается взять `mutex` `m1`, а в `thread_proc` происходит наоборот: берётся `m1`, вызывается `on_next_number`, который пытается взять `m2`.

Одно из решений: объединить эти `mutex`'ы в один, другое: заметить, что при вызове `on_next_number` не требуется удерживать `m1`, но для этого нужно копировать `current_val`, а это может быть дорого или вообще запрещено в случае более сложного объекта.

Пример может показаться надуманным, но такое возникает достаточно часто, если писать многопоточный код, разбивая на классы. Одна из формулировок, которую используют: "не нужно делать коллбэки (в примере это функция `on_next_number`), удерживая мьютекс", другая: "то, какие мьютексы удерживает класс — часть интерфейса".\
Интересно, что в примере оба класса по отдельности корректны, но вместе вызывают взаимоблокировку. Получается, что рассуждая о корректности программы, нужно знать, что генератор, вызывая коллбэк, держит тот же внутренний мьютекс, что и блокирует `reset`.

## Атомики.

В начале лекции мы говорили о том, что многопоточный доступ (с записями) к обычным данным это UB. Почему так было специфицировано? Ведь на самом деле в архитектуре процессора написано, что должно произойти.

Во-первых, на разных архитектурах происходят разные вещи. Например, поскольку процессор может переупорядочивать инструкции, он может переупорядочивать записи и чтения. И на разных процессорах можно получить самые разные мемы. Есть даже [весёлая статейка](http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.06.07c.pdf), в которой на 16 странице есть табличка о том, какие архитектуры что могут переупорядочивать. И эта табличка очень сильно упрощена, потому что почти все архитектуры имеют разные команды, которые позволяют разную степень переупорядочивания.\
Так вот, поэтому нельзя специфицировать многопоточное поведение для встроенных типах: когда процессор переупорядочит инструкции, ваше специфицирование вам же выйдет боком. Причём даже если у вас всё хорошо, и есть специальные многопоточные инструкции, то компилятор не знает, многопоточное ли у вас приложение, или нет, ему придётся использовать тяжёлые многопоточные инструкции всегда.

Окей, ладно, так что за такое атомарность и зачем оно надо? **Атомарная операция** *— неделимая с точки зрения других потоков (с их точки зрения она либо началась, либо завершилась).*\
И вот что иногда хочется — так это атомарно увеличить значение переменной. Такие операции нередко и в процессорах есть (например, на x86 такая операция называется `lock add`). Но вообще атомарной может быть любая операция, например, [обмен со сравнением](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange), о котором мы поговорим чуть ниже.

Стоит заметить важный момент про реализацию atomic'ов. Для них обычно очень критично, чтобы данные были выровнены (попробуй, например, атомарно поменять значение, лежащее на границе кэш-линий). Впрочем, в x86 возможно атомарно работать с двумя кэш-линиями сразу, но со спецэффектами уровня "другие ядра не могут читать из памяти в этот момент" (см. [split lock](https://lwn.net/Articles/790464/)).

### Простой пример работы с `std::atomic`.

В C++ есть класс `std::atomic` со специализациями для всех встроенных (и не только) типов, предоставляющий атомарный доступ к переменным.\
Давайте подумаем, как можно переписать наш пример с банковскими счетами, чтобы он работал через `std::atomic`. Нам надо как-то сделать так, чтобы атомарно работала пара из сравнения и вычитания. И для такого есть те самые обмены со сравнением:

```c++
std::array<std::atomic<uint32_t>, 10'000> accounts;

void transfer(size_t to, size_t from, uint32_t amount) {
    if (from == to)
        return;

    uint32_t old = accounts[from].load();
    do {
        if (old < amount)
            throw std::runtime_error("insufficient funds")
    } while (!accounts[from].compare_exchange_weak(old, old - amount));
    accounts[to] += amount;
}
```

Операция `compare_exchange` принимает два аргумента: `expected` и `desired` — и записывает в `atomic` значение `desired`, если там записано значение `expected`, иначе в `expected` записывается значение из `atomic`'а.

Есть `compare_exchange_weak` и `compare_exchange_strong`: `weak`-форме разрешено "спонтанно фейлиться" (вести себя так, будто `*this != excepted`, даже если они равны), `strong`-форма гарантирует ожидаемое поведение (внутри там что-то вроде цикла). На x86 разницы между ними нет, там обе формы выражаются одной ассемблерной инструкцией, но на POWER'ах, например, `compare_exchange_strong` является циклом, в теле которого происходит `compare_exchange_weak`, поэтому если у вас в программе и так написан цикл, лучше использовать `compare_exchange_weak`.

### Relaxed atomic'и и `memory_order`.

Почти у всех операций с `atomic`'ами есть дополнительный параметр `memory_order`, у которого всегда есть значение по умолчанию (`seq_cst`). Возникает резонный вопрос, что это такое. На эту тему рекомендуется посмотреть [C++ and Beyond 2012: Herb Sutter - atomic Weapons 1 of 2](https://www.youtube.com/watch?v=A8eCGOqgvH4), но мы вкратце перескажем самые важные для нас моменты доклада.

Обычно (т.е. с этим самым параметром `seq_cst`) все операции гарантируют, что их можно чётко расположить на временной оси и определить порядок. Но реальное железо работает совсем иначе (у нас есть кэши разных уровней, какие-то из уровней общие для нескольких ядер и т.д). В такой системе дорого поддерживать иллюзию. Кстати, на x86 процессор почти полностью поддерживает её сам: там что бы вы не делали, все `std::memory_order`'ы работают одинаково. Но не на ARM'ах или POWER'ах, где все они [порождают разные инструкции](https://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html).

Окей, но всё же, кто такие эти ваши не `seq_cst` (не sequential-consistent) операции? А представим вот такой пример (опять же, с точки зрения C++ это UB, но мы делаем вид, что мы на ASM это написали, а не на C++):

```c++
int x = 0, y = 0;

void thread_1() {
    x = 1;
}

void thread_2() {
    y = 1;
}

void thread_3() {
    int x3 = x;
    int y3 = y;
    if (x3 == 1 && y3 == 0)
        std::cout << "x is written to before y\n";
}

void thread_4() {
    int y4 = y;
    int x4 = x;
    if (x4 == 0 && y4 == 1)
        std::cout << "y was written to before x\n";
}
```

Может ли такое произойти, что `thread_3` и `thread_4` оба выведут свои сообщения? Казалось бы, нет. И в случае работы со стандартными `std::atomic`'ами с указанием `std::memory_order::seq_cst` действительно будет нет. Но на самом деле у вас могут ядра с потоками 1 и 3 быть соединённым одним общим кэшом, а с потоками 2 и 4 — другим. Тогда потоки 3 и 4 оба увидят только нужное им изменение.

Окей, но я же сказал, что в x86 всё хорошо. То есть там такой пример не работает, да?  Да. *Конкретно такой* пример на x86 работает нормально. Но в мануале есть интересная фраза "Any two stores are seen in a consistent order by processors other than those performing the stores" (см. [Intel® 64 and IA-32 Architectures Software Developer’s Manual Combined
Volumes: 1, 2A, 2B, 2C, 2D, 3A, 3B, 3C, 3D, and 4](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html) страница 3094, раздел 8.2.3.5, пример 8-5). Или еще тот же пример можно увидеть тут: [Memory Reordering Caught in the Act](https://preshing.com/20120515/memory-reordering-caught-in-the-act/).

То, в каком порядке ядро читает из памяти и пишет в неё становится непредсказуемым из-за оптимизаций. Конкретно в x86 правила такие: чтения не переупорядочиваются с другими чтениями, записи не переупорядочиваются с чтениями, идущими до них, записи не переупорядочиваются с другими записями кроме некоторых исключений, *чтения могут быть переупорядочены с идущими до них записями в другие места памяти*. На эту тему есть [забавный пример](https://www.realworldtech.com/forum/?threadid=173441&curpostid=192262) о том, что даже в однопоточной программе в зависимости от порядка инструкций меняется производительность, потому что процессор обязан соблюдать порядок инструкций. Для борьбы с перестановкой инструкция применяются так называемые [барьеры памяти](https://vedmysh.livejournal.com/5558.html).  

Так, ну, ладно, но мы тут уже несколько страниц расписали, а про `memory_order`'ы так ничего и не сказали. Так вот что это: это `atomic`'и с более слабыми гарантиями, которые на части систем работают быстрее. Вот они, как в известной притче, слева направо:

- Если один поток сделал запись с `release`, а второй — чтение с `acquire`, то все чтения, сделанные до записи, будут видны во втором потоке. Можно применить например тогда, когда мы имеем две переменных: значение и флаг, посчитано ли оно.

    ```c++
    std::string value;
    std::atomic<bool> value_present;

    void produce() {
        value = "hello";
        value_present.store(true, std::memory_order_release);
        // ... 
    }

    void try_consume() {
        if (value_present.load(std::memory_order_acquire)) {
            std::string tmp = value;
            // ...
        }
    }
    ```

    Пример выше без них накладывает больше ненужных гарантий, которые как раз ослабляются параметром `memory_order`.\
    Если посмотреть, во что это транслируется, то будет видно, что с дефолтным `memory_order` запись делается инструкцией `xchg`, а в случае с `memory_order_release`, там будет `mov`.

- `memory_order_relaxed` гарантирует только атомарность и никаких других гарантий. Применимо, если вам нужен какой-то счётчик, а упорядоченности вы достигнете сами, дождавшись завершения потока, например.

    ```c++
    void thread_proc() {
        for (;;) {
            // ...
            number_of_events.fetch_add(1,   std::memory_order::relaxed);
            // ...
        }
    }

    int main() {
        number_of_events.store(0, std::memory_order::relaxed);
        std::thread th1(&thread_proc);
        std::thread th2(&thread_proc);
        // ...
        th1.join();
        th2.join();
        number_of_events.load(std::memory_order::relaxed);
    }
    ```

    Также `memory_order_relaxed` можно использовать при записи в память до создания других потоков, потому что  их создание так же гарантирует, что они увидят все записи, сделанные до этого.

- `memory_order_acq_rel` — комбинация `acquire` и `release`, применяется в некоторых операциях, которые и читают, и пишут.
- А кто же такой `memory_order_consume`? Вообще это нечто среднее между `acquire` и `relaxed`. Нужен он бывает редко, поддерживается не всеми, так что фиг с ним, неважно, кто это.

### Worst practices.

Никогда, слышите, никогда не используйте один `memory_order` вместо другого, опираясь на то, что на вашей архитектуре они одно и то же. Это не только очень платформозависимо, это ещё и от компилятора зависит, потому что он тоже имеет право инструкции переставлять. Поэтому руководствуйтесь только свойствами языка.

### Ненативные атомики.

Вообще в `std::atomic` можно запихнуть любой тривиально копируемый (и просто перемещаемый) тип. Но не для любого же размера процессор имеет инструкции. Ну, да. Поэтому для некоторых типов `std::atomic` будет скрафчен руками на основе `mutex`'ов. Это даже [проверить](https://en.cppreference.com/w/cpp/atomic/atomic/is_always_lock_free) можно.

### Цена атомиков. Sharing.

Рассмотрим следующую программу:

```c++
#include <boost/program_options.hpp>
#include <atomic>
#include <iostream>
#include <thread>
#include <vector>
#include <memory>
namespace po = boost::program_options;

void thread_proc(std::atomic<int>& v, std::atomic<bool>& finish) {
    while (!finish.load())
        v += 1;
}

int main(int argc, char* argv[]) {
    try {
        po::options_description desc("Allowed options");
        desc.add_options()
            ("threads,j", po::value<size_t>(), "set number of threads")
            ;
        po::variables_map vm;
        po::store(po::parse_command_line(argc, argv, desc), vm);
        po::notify(vm);

        size_t number_of_threads = 1;
        if (vm.count("threads") != 0)
            number_of_threads = vm["threads"].as<size_t>();

        for (;;) {
            std::cout << number_of_threads << " threads";

            std::atomic<int> val(0);
            std::atomic<bool> finished(false);

            std::vector<std::thread> threads;
            for (size_t i = 0; i != number_of_threads; i++)
                threads.emplace_back(&thread_proc, std::ref(val), std::ref(finished));

            std::this_thread::sleep_for(std::chrono::seconds(2));
            finished.store(true);

            for (auto i = threads.rbegin(); i != threads.rend(); i++) {
                auto& th = *i;
                th.join();
            }

            std::cout << ", " << val.load() << " iterations" << std::endl;
        }
    } catch (std::exception const& e) {
        std::cerr << e.what() << std::endl;
        return EXIT_FAILURE;
    }
}
```

Эта программа принимает аргументом число `N` и создаёт `N` потоков, которые внутри себя инкрементят переменную `val` в цикле, пока их не остановят через 2 секунды. В итоге выводим сколько раз за 2 секунды `N` потоков инкрементят переменную `val`. Интересно, как при увеличении числа потоков изменится значение переменной.

При запуске на одном потоке результат получился в 4 раза больше, чем на двух потоках, при этом на двух потоках результат получился "шумным" (скачет от 2 до 4 раз). При увеличении числа потоков дальше результат остаётся примерно одинаковым. 

Дело в том, что работать из разных потоков с одной переменной одновременно всё равно нельзя — инкременты у процессора будут выполняться последовательно, поэтому получить результат лучше, чем на одном потоке, не получится. Почему результат для двух потоков был шумным и в несколько раз хуже? Ответ кроется в кэшах — запись в переменную от одного ядра инвалидирует её в других кэшах. Проблема, с который мы тут столкнулись, называется **sharing**.

С помощью команды *taskset* можно ограничить ядра, на которых запускается программа. Например, если с двумя потоками двух логических ядрах, которые на самом деле одно физическое, то результат примерно такой же, как если запускать программу в один поток (потому что кэш один и тот же).

Один из способов решения такой проблемы — делать по переменной на каждый поток, а в конце складывать результаты. Давайте заведём `N` этих переменных:

```c++
std::vector<std::atomic<int>> vals(number_of_threads);
```

И каждому потоку выдадим по своей переменной.

Кажется, что должно стать лучше, ведь теперь все потоки пишут в разные переменные, но результат получился примерно тот же. Проблема опять в кэше — соседние переменные из вектора попадают в одну кэш-линию. Такая ситуация называется **false sharing** (как и в принципе когда две переменные в одной кэш-линии лежат).

Можно аккуратно подобрать отступы так, чтобы переменные для разных потоков попадали в разные кэш-линии. Размер кэш-линии на x86 64 байта, размер атомарного `int`'а — 4 байта, поэтому кажется, что надо делать сдвиг на 16. Тогда при запуске на разных ядрах двух потоков результат получается в 2 раза больше, чем при запуске на одном ядре. 

Если покрутить значения, то заметно, что при сдвиге меньше 8 на двух потоках результат как на одном, при сдвиге больше 8 результат на двух потоках стабильно в 2 раза больше. Откуда берётся число 8, если мы посчитали, что сдвиг должен быть равен 16? Вектор выделяет память, выравнивая по 32 байта, поэтому он мог выделить память так, что первые 8 элементов попали в одну кэш-линию, а следующие 8 уже в следующую, поэтому и было достаточно сдвига 8.

### Насколько применимы не-`seq_cst` атомики?

Есть версия, что их не надо использовать по причине их неинтуитивности. Причём иногда максимально неинтуитивная. Можете открыть [презентацию о том, как атомики пихали в CUDA](https://youtu.be/VogqOscJYvk?t=3212) и найти там несколько примеров. Там модель памяти чем-то похожа на POWER и 32-битный ARM. И эти чуваки из NVIDIA начали проверять свою трансляцию формальными методами: они брали пример плюсовой программы, смотрели, как позволяет стандарт и сравнивали со своей трансляцией. И столкнулись они с тем, что стандарт и трактовка большинства компиляторов не согласована (компиляторы слишком много себе позволяют). После этого стандарт пофиксили, но пример всё ещё очень показателен: куча чуваков, которые очень сильно шарили в многопоточке, не замечали багов стандарта очень долго. Поэтому очень сильно думайте, насколько вам нужны relaxed-атомики, особенно в комплексных случаях.

Да и вообще, писать многопоточку сложно. Вы не можете найти случай, когда программа не работает, например. Тут вам помогут *thread-sanitizer*'ы, *cppmem* и средства формальной 
проверки, но боже, оно вам точно надо?

## `volatile`.

### Исторический контекст.

Многопоточка появилась в C++11. Но многопоточные программы писались и до этого средствами ОС и библиотек. И некоторые библиотеки для разных целей использовали `volatile`.

В интерпретации стандарта `volatile` к многопоточке не имеет никакого отношения. А к чему имеет?

### Применения `volatile`.

У нас могут быть какие-то устройства, которые тупо отображаются в память по каким-то адресам. Вы делаете запись, а это не честная запись в память, а интерфейс для взаимодействия с устройством. И вполне нормальное применение — читать несколько раз подряд. Любой нормальный компилятор офигеет от такого и заменит все чтения на одно. Для обычной памяти это норм, но вот в случае с устройствами нам такое не надо. Это первое применение `volatile`.

Второй случай — `setjmp` и `longjmp`. На C++ вам это не надо, но вообще это в некотором смысле C'шный аналог исключений. И там тоже при записи в переменную после `setjmp` может иметь смысл делать `volatile`.

И третий случай — UNIX-сигналы (ничего общего с [сигналами, которые мы обсуждали](./23_signals_reetrancy.md)). Вас на любой инструкции могут дёрнуть и сказать разные интересные вещи. И поскольку вас могут позвать на любую инструкцию, делать обработчики UNIX-сигналов могут не всё. Например, могут обращаться к `volatile`-переменным. Это более применимый случай, но тоже довольно нишевый.

### Почему люди ошибаются.

Почему у людей возникает сомнение в том, что `volatile` не применимо в C++?
- Во-первых, `volatile` в других языках имеет другой смысл: например, в Java это именно что атомики.
- Во-вторых, строго говоря, набор ограничений на `volatile` переменные похож на те, которые накладываются на атомики, но они [не одинаковы](#почему-это-не-атомики).
- В-третьих, до C++11 и в C `volatile` использовался при работе с потоками. И, например, в MSVC есть ключик для совместимости со старыми программами, заменяющий `volatile`-переменные на атомики.

### Почему это не атомики.

Набор ограничений на `volatile`-переменные и на атомики похож, но всё же [много когда отличается](https://www.drdobbs.com/parallel/volatile-vs-volatile/212701484). Например:

```c++
volatile int a;
void foo() {
    a = 1;
    a = 2; // компилятор обязан сохранить порядок записи
}

std::atomic<int> b;
void bar() {
    b.store(1);
    b.store(2); // компилятор имеет право оставить только запись 2, сделав вид, что другой поток не успел прочитать между записью 1 и 2
    b.load(); // может не читать
}

void progress_bar() {
    for (size_t i = 0; i != 10000; ++i) {
        // ...
        b += 1;
    }
    // может быть соптимизировано в b += 10000; снаружи цикла
}
```

Но при этом обычно компилятор не оптимизирует такие штуки,  потому что это может испортить смысл программы (как в примере с `progress_bar`), но тем не менее возможность у него такая есть (и иногда это даже полезно). Подробнее про это можно прочитать [No Sane Compiler Would Optimize Atomics](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4455.html) или [When should compilers optimize atomics](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0062r1.html).

### Мораль.

[Применим ли `volatile` с потоками в C++?](https://isvolatileusefulwiththreads.in/cxx/)

## Части вашей жизни, которые меняются в многопоточности.

Самый канонический пример, когда многопоточность меняет жизнь, — аллокаторы памяти. Раньше, когда процессоры были одноядерные, проблема нескольких потоков была проблемой корректности, а не производительности, поэтому в аллокаторах памяти просто все операции оборачивали в `mutex`'ы. Но потом в многоядерных системах это стало приводить к проблемам: если два потока аллоцируют и освобождают память в процессе работы, при этом не работая с общими данными, то аллокатор будет узким местом.

Поэтому стали появляться многопоточные аллокаторы. Один из первых таковых — это [Hoard](http://hoard.org/). Хоть он и устарел и на практике уже давно не применяется, концепции, описанные там, переняли разработчики других аллокаторов. Они попытались сделать так, чтобы аллокатор хорошо масштабировался на много потоков, при этом пытаясь избежать false sharing'а памяти, которую возвращает аллокатор (например, если аллокатор выделил два куска памяти для разных потоков, они не должны попадать в одну кэш-линию).

Концептуально они добились этого тем, что сделали `N` копий однопоточного аллокатора, назначая каждому потоку аллокатор по хешу потока, в итоге потоки равномерно распределяются по аллокаторам, кроме того случая, когда один поток выделил память, а другой освобождает её (в таком случае освобождать должен аллокатор первого потока). Заведение нескольких копий позволило решить проблему false sharing'а: нужно просто гарантировать, что каждая кэш-линия принадлежит куче ровно одного из аллокаторов. Ещё один трюк: если поток обращается к аллокатору, а тот заблокирован, он может переходить к следующему и так, пока не найдёт свободный, который он запомнит и будет дальше обращаться только к нему. Подробнее про всё это можно почитать в [пейпере по Hoard](https://people.cs.umass.edu/~emery/pubs/berger-asplos2000.pdf).

## Cancellation.

Почти наверняка, когда запускаем какой-то поток или даём какому-то потоку сделать какую-то операцию, от него хочется получить какой-то результат. Есть проблема: может произойти так, что в момент, когда операция завершится, её результат уже не будет никому нужен, тогда возникает несколько вопросов: например, не получится ли так, что когда операция завершилась, поток обращается к каким-то несуществующим объектам, или из программы вышли совсем.\
Как мы помним, опции убить поток не существует, поэтому сущетсвует техника **cancellation**: сообщить потоку, что он больше не нужен и может выходить. Например, это можно сделать через `std::atomic<bool>`.

В C++20 такое стандартизовали, появился `std::jthread` с похожим функционалом. У него есть `stop_token`:

```c++
int main() {
    std::jthread jt{ [](std::stop_token st) {
        while (!st.stop_requested()) {
            // ...
        }
    }};
    sleep(5);
    jt.request_stop();
    jt.join();
}
```

`request_stop` устанавливает `stop_token`. Деструктор `jthread` так же вызывает `request_stop`, если он не был вызван до удаления объекта, и делает `jt.join()`. Кстати, тут видно, что удобно думать о потоках как об обычных ресурсах.

Вроде как, если достаточно часто проверять `stop_token`, то всё будет хорошо. Но на самом деле это не так.\
Дело в том, что у нас есть блокирующие операции: чтение из сокета, например. Или ещё более банальный пример: `std::mutex::lock` или `std::condition_variable::wait`. Во время таких операций не получится прервать поток.

В общем случае с блокирующими операциями ничего сделать нельзя (нужно пользоваться механизмами, которые предоставляет ОС). Но в некоторых конкретных соучаях что-то сделать можно. Например, у `std::condition_variable::wait` есть перегрузка, принимающая `std::stop_token`. А ещё есть такая штука как `std::stop_callback`, которая регистрирует обработчик на `std::stop_token`. Если вам чем-то это поможет, конечно.

### Асинхронные операции на Linux.

Вместо блокирующих операций можно использовать асинхронные операции, которые более-менее независимо появились в UI и в серверных программах. 

Как работает сервер: у него есть *n* клиентов, из каждого из которых он хочет читать, поэтому просто так делать блокирующие операции не получается. Традиционно делали следующее: каждому соединению заводили поток, что не очень экономно.

Альтернативный подход - спросить у ОС, у какого из сокетов (или файловых дескрипторов), можно "взять данные", такой механизм называется `poll`, он пробегается по всему массиву файловых дескрипторов и проверяет, готовы ли они. В целом это ок, но есть недостаток: когда дескрипторов много, а готовы из них два–три, нам всё равно надо пробежаться по здоровому массиву.\
Проблемы с производительностью решил `epoll`, в который можно не сразу передать *n* дескрипторов, а добавлять их или убирать по одному. В отличие от `poll`, он возвращает список только тех дескрипторов, для которых произошли события.\
Подробнее и понятнее можно почитать в [статье на хабре](https://habr.com/ru/company/infopulse/blog/415259/).

Операции такого рода называются **асинхронными**. Преимущества таких операций в том, что не нужно создавать много потоков, можно самостоятельно приоритизировать сокеты.

Кроме сетевых программ, асинхронные операции нужны в UI-программах. Различные события (нажатие кнопки, клик мышки и т.д.) складывают в одну очередь, не заводя отдельные потоки. Часто внутри там тоже `epoll`. Но это мы подробнее обсудим, когда будем [детальнее говорить про QT](./28_qt.md).

*Тут был пример UI-программы на QT, смотрите [запись](https://youtu.be/SiGqozCBXDE?t=1627)*.

### Про блокирующие операции и cancellation.

Пусть есть поток, который повис на одной из блокирующих операций. Как убить такой поток? Поскольку все эти операции зависят от ОС, то и механизмы cancellation'а специфичны для ОС. 

Просто пример - сокеты. У них есть функция `shutdown`, которая отменяет вызов `recv` на сокете. После её вызова, с сокетом работать нельзя. **Важно**: не нужно пытаться вызывать `close`, это не отменит блокирующую операцию.

Пусть есть поток, который сидит в вызове `sleep`. Как его прервать? Прямого способа нет, поэтому нужно начать издалека и поговорить про [UNIX-сигналы](https://man7.org/linux/man-pages/man7/signal.7.html) (не надо путать с [сигналами](./23_signals_reetrancy.md), о которых говорили на прошлых лекциях). Сигналы похожи на прерывания, но внутри одного процесса. Например, обращение по нулевому указателю, программа прерывается с сигналом `SIGSEGV`. На самом деле, программа может зарегистрировать функцию, которая называется обработчик сигнала и будет вызвана на него. Обработчик сигнала может быть вызван на любой инструкции, поэтому  он не может полагаться на какое-либо состояние программы, вследствие чего внутри обработчика сигнала можно пользоваться ограниченным набором системных вызовов.

```c++
void sigsegv_handler(int) {
    int a = 5;
}
int* p;
int main() {
    struct sigaction new_action;
    new_action.sa_handler = &sigsegv_handler;
    sigemptyset(&new_action.sa_mask);
    new_action.sa_flags = 0;
    sigaction(SIGSEGV, &new_action, nullptr);
    *p = 42; // SIGSEGV
}
```

Если потоку приходит сигнал, то системные вызовы либо дорабатывают до начала обработки сигнала, либо возвращают код ошибки. Это сделано для того, чтобы избежать ситуации, когда обработчик сигнала вызывают системный вызов, на инструкции которого он уже был вызван.

#### Прикол про pthread cancellation.

На линуксе есть ещё одно API:

```c++
void thread_proc() {
    for (;;) {
        pthread_testcancel();
    }
}
int main() {
    pthread_cancel(handle);
}
```

Как это работает? Вот в C++20 есть `stop_token`, но везде его с собой таскать неудобно. Поэтому в POSIX это `thread_local`-переменная, которую и проверяет `pthread_testcancel()`. При этом формально `pthread_testcancel()` при необходимости прерваться делает нечто похожее на бросок исключения (например, в glibc это буквально исклчение `__abi::force_unwind`; читатели конспектов помнят, что его [нельзя ловить](./08_exceptions.md#catch--и-throw)).

Его можно использовать самостоятельно, но в куче блокирующих операций из стандартной библиотеки эта проверка уже стоит (например, в `read`). Применяется это как-то так:

```c++
int main() {
    std::thread th([]{
        for (;;) {
            std::string s;
            std::cin >> s; // блокирующая, внутри есть pthread_testcancel
            // вылетает abi::__forced_unwind
            if (!std::cin) {
                break;
            }
            std::cout << s << '\n';
        }
    });
    std::this_thread::sleep_for(std::chrono::seconds(1));
    pthread_cancel(th.native_handle());
    th.join();
}
```

Несмотря на то, что это удобное API, программисты на C не знают, что такое исключения, поэтому не используют его. А там прям всё, как в исключениях:

- Можно сделать `pthread_set_cancelstate`, который позволяет сделать вам аналог noexcept (т.е. сделать так, чтобы какой-то кусок кода не реагировал на прерывание). В C++, кстати, для этого есть конструктор `std::stop_token` по умолчанию.
- Есть деструкторы (но придётся руками их делать через `pthread_cleanup_push` и `pthread_cleanup_pop`).

Разумеется, программисты на C не знают ничего про гарантии исключений, про RAII и про все остальные связанные с исключениями темы, так что они не пользуются POSIX cancellation'ом, а значит его никто нормально и не поддерживает. Обидно.
